<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cinjon</title>
    <description>Cinjon&#39;s on the internet
</description>
    <link>http://cinjon.com/</link>
    <atom:link href="http://cinjon.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 09 Nov 2015 15:00:05 -0500</pubDate>
    <lastBuildDate>Mon, 09 Nov 2015 15:00:05 -0500</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Making Html of Images</title>
        <description>&lt;p&gt;Here’s an idea.&lt;/p&gt;

&lt;p&gt;Let’s say you have lots of PDFs and you want to manipulate them. Some example tasks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Extract table data&lt;/b&gt;:&lt;/p&gt;

    &lt;p&gt;PDFs frequently have tabular information. Sometimes, this is explicit where the tables are delineated with lines. Other times, it is implicit like in a table of contents. Regardless, it isn’t easy to get this information. There are a number of libraries that try. The best open source one I’ve found is &lt;a href=&quot;http://www.tabula.technology&quot;&gt;Tabula&lt;/a&gt;. None of them, however, work reliably enough so that we can pipe many PDFs at once through the system and expect consistently solid results.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Convert images to text&lt;/b&gt;:&lt;/p&gt;

    &lt;p&gt;It’s common for PDFs to have images. Sometimes, this is innocuous like an embedded png that supports the document text (like a diagram). Frequently it is not so simple like when entire pages are images. They might even &lt;i&gt;look&lt;/i&gt; like normal pages but they can’t be selected, copied, or searched. Every audience hates this. Converting these images to their equivalent in text makes it much easier to access.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Translating the document&lt;/b&gt;:&lt;/p&gt;

    &lt;p&gt;Translating a PDF means extracting the text into coherent “blobs” so that structures like paragraphs remain together and not split by line. It means then taking those blobs and translating them into a target language before reproducing the document as faithfully as possible. This is easy when done with HTML - the blobs are already together and the spacing is already determined - but is very difficult when the source document is as loose as a PDF. There, concepts like blobs just don’t exist on the right granularity. And so any effort to reliably reproduce the document is very hard.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these are gritty problems that have existed for years without great solutions.&lt;/p&gt;

&lt;p&gt;Imagine if the PDF was instead an image. We are ignoring the instructions given in the PDF in favor of utilizing image processing software. This makes it quite easy to find and extract table positions but quite hard to extract non-romanic text. The former is just about looking for grid lines and can be done in Python with OpenCV. The latter needs OCR software. Outside of romanic text, open-source implementations are not very good [yet].&lt;/p&gt;

&lt;p&gt;But what if we could skip developing an OCR algorithm for every language? Instead, we train a model to understand how to generate images with html. We have a lot of data - every web page on the internet suffices. Step one would be to gather a lot of website htmls with their stylesheets. Then, munge the two together so that the stylesheets are inserted into the html. Now take images of the resulting page.&lt;/p&gt;

&lt;p&gt;The input to our model is the images and the output is the merged html. We train it to understand the how to produce the html from the image. It’s not clear to me that this would be advancing the frontier of research or need anything beyond what we have today. It &lt;i&gt;does&lt;/i&gt; look like a very useful application that utilizes advances in understanding long-term memory dependencies.&lt;/p&gt;

&lt;p&gt;As a quick example of that, what if the entire page is just two columns? Then the resulting html could be a parent div with two child divs, each of which are fixed percentage width and floated left. The model has to learn that this large amount of information should all be in this one dom element. And if there is a white space separation some length down the page, then the model needs to grasp that.&lt;/p&gt;

&lt;p&gt;An amusing thought is to see what happens when you train it on different html structures. We could have a Bootstrap bot, a Shopify bot, and a Google bot that all strive to turn images into pages that look like how sites with those signature frameworks are displayed.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Nov 2015 00:00:00 -0500</pubDate>
        <link>http://cinjon.com/making-html-of-images</link>
        <guid isPermaLink="true">http://cinjon.com/making-html-of-images</guid>
        
        <category>html,pdfs</category>
        
        
      </item>
    
      <item>
        <title>Papers Leveraging Monolingual Corpora</title>
        <description>&lt;link rel=&quot;stylesheet&quot; href=&quot;/public/katex.min.css&quot; /&gt;

&lt;script src=&quot;/public/katex.min.js&quot;&gt;&lt;/script&gt;

&lt;style&gt;
  .MathJax_Preview { color: #888 }
  .math_finished .MathJax_Preview { display: none }
&lt;/style&gt;

&lt;p&gt;Here’s another set of papers! These ones are all about utilizing the tremendous amount of monolingual corpora out there. One of my interests here is in applying them to aid our comprehension across languages.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1504.07225&quot;&gt;Correlational Neural Networks&lt;/a&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;Common Representation Learning is the task of embedding different descriptions of data into a common subspace. An example is two sentences in different languages that mean the same thing. It would be good to reliably be able to associate one with the other without explicitly telling a machine that, for example, “cats” and “dogs” are alike.&lt;/p&gt;

    &lt;p&gt;This paper does this a deep autoencoder model with a well-considered objective function. It then applies this model to tasks like cross-language document classification where a document is represented by the tf-idf sum of its bag of words representation. Here’s the objective function it’s trying to minimize:&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\displaystyle J(\theta) = \sum_{i=1}^N L(z_i, g(h(z_i)))+L(z_i, g(h(x_i)))+L(z_i, g(h(y_i)))-\lambda corr(h(X), h(Y))&lt;/script&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\displaystyle corr(h(X), h(Y))=\frac{\sum_{i=1}^N (h(x_i) - \overline{h(X)})(h(y_i) - \overline{h(Y)})}{\sqrt{\sum_{i=1}^N (h(x_i) - \overline{h(X)})^2 \sum_{i=1}^N (h(y_i) - \overline{h(Y)})^2}}&lt;/script&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;Above, &lt;code&gt;L&lt;/code&gt; is the reconstruction loss. They use cross-entropy error if the input takes binary values and squared error loss otherwise. &lt;code&gt;z_i&lt;/code&gt; is the paired inputs from the different descriptions (sentences), &lt;code&gt;(x_i, y_i)&lt;/code&gt;, and &lt;code&gt;h(z) = f(Wx + Vy + b)&lt;/code&gt;, where &lt;code&gt;f&lt;/code&gt; is a non-linear activation function. &lt;code&gt;W&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; are the learned projection matrices for the different descriptions, and &lt;code&gt;b&lt;/code&gt; is a learned bias vector. Then &lt;code&gt;z&#39; = g([W&#39;h(z), V&#39;h(z)] + b&#39;)&lt;/code&gt; is the reconstruction that we compare to &lt;code&gt;z&lt;/code&gt; while learning reconstruction matrices &lt;code&gt;W&#39;&lt;/code&gt;, &lt;code&gt;V&#39;&lt;/code&gt;, and &lt;code&gt;b&#39;&lt;/code&gt;. Function &lt;code&gt;g&lt;/code&gt; is any activation function. Note that &lt;code&gt;[a, b]&lt;/code&gt; is the concatenation of vectors &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;

    &lt;p&gt;The sum is made of four parts. The first part, &lt;code&gt;L(z_i, g(h(z_i)))&lt;/code&gt;, teaches the autoencoder to reconstruct its input (both languages). The second and third terms train it to reconstruct the input from just one description. So, if we are training on French and English, it must learn to output the French language with just the English sentence. These three together imply that the model learns to embed the sentences in the same vector space. The fourth term maximizes the correlation between the vector embeddings of similar sentences across languages. This constrains the vectors in different languages to be close to each other. Pretty slick addition.&lt;/p&gt;

    &lt;p&gt;An important note is that the inputs are sentence-level binary bags of words. This means that word order is not taken into effect and changing this to include word order would be a reasonable next thing to try.&lt;/p&gt;

    &lt;p&gt;Overall, I really like this idea. It’s simple but makes sense and the results suggest that it’s working well for document classification. They are able to assemble deep models and can scale up the language representation with lots of monolingual data without compromising the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1412.6334&quot;&gt;Leveraging Monolingual Data for Crosslingual Compositional Word Representations&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;A couple of months before the above paper, this one achieved even better results on document classification with a different method and a slightly different objective.&lt;/p&gt;

    &lt;p&gt;The goals are similar. Can a single method:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Contrain the word-level representations to be compositional?&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Leverage both monolingual and bilingual data.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Scale to large vocabulary sizes without greatly impacting time.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;While very favorable towards the [Chandar et al] paper, they describe a deficiency:&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;[Chandar et al]’s method represents each sentence as a bag of words vector with the size of the whole vocabulary. This leads to computational scaling issues and necessitates a vocabulary cut-off which may hamper performance for compounding languages such as German.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;Accordingly, their neural network representing sentences as an addition composition over word vectors can be trained on seven million sentences in under six hours on a single core CPU. They did show a composition function using bigrams to somewhat consider word order, but left it as future work:&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;To increase the expressiveness of our method we plan to investigate more complex composition functions, possibly based on convolution to preserve word order information.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;I mentioned a different objective as well. Here it is:&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\displaystyle L_{total} = \sum_{i=1}^{N_{bi}} L_{bi}(v_{i}^{l_1}, v_{i}^{l_2}) + \sum_{i=1}^{N_{mono_1}} L_{mono}(x_{i}^{l_1}) + \sum_{i=1}^{N_{mono_2}} L_{mono}(y_{i}^{l_2}) + \lambda||\theta||^2&lt;/script&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;The first term is the bilingual objective. It’s calculated by taking aligned sentences, computing their vector representation with the composition function, and then minimizing the squared euclidean distance between them:&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\displaystyle L_{bi}(v_i^{l_1},v_i^{l_2}) = ||v_{i}^{l_1} - v{i}^{l_2}||^2&lt;/script&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;The monolingual objective is very different. It’s based on a notion that phrases are more similar to their sub-phrases than to randomly sampled phrases. For example, if the phrase is &lt;code&gt;My friend went to the park after getting ready,&lt;/code&gt; then the sub-phrase &lt;code&gt;went to the park&lt;/code&gt; is more relevant than &lt;code&gt;took a shower.&lt;/code&gt; Given a large enough monolingual corpus, another plausible sub-phrase like &lt;code&gt;went to the movies&lt;/code&gt; would also appear and be embedded close to the larger phrase. This would consequently embed it close to &lt;code&gt;went to the park.&lt;/code&gt; Here is the loss function that achieves this:&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\displaystyle L_{mono}(a)=[max(0, m + ||a_c^{outer} - a_c^{inner}||^2 - ||a_c^{outer} - b_c^{noise}||^2) + ||a_c^{outer} - a_c^{inner}||^2] \frac{len(a^{inner})}{len(a^{outer})}&lt;/script&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;Above, &lt;code&gt;m&lt;/code&gt; is a margin, the &lt;code&gt;outer a&lt;/code&gt; is a phrase sampled from a sentence, the &lt;code&gt;inner a&lt;/code&gt; is a sub-phrase of the outer, and the &lt;code&gt;noise b&lt;/code&gt; is a phrase extracted from a sentence uniformly sampled from the corpus. All extracted phrases have length larger than three.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1506.06726v1.pdf&quot;&gt;Skip-Thought Vectors&lt;/a&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;This work was inspired by Mikolov et al’s &lt;a href=&quot;http://arxiv.org/pdf/1301.3781.pdf&quot;&gt;paper&lt;/a&gt;&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; in 2013. It abstracts the skip-gram model for word embeddings to the sentence-level. The result are generic sentence representations that perform robustly across a wide variety of tasks, including semantic relatedness, paraphrase detection, image-sentence ranking, and classification.&lt;/p&gt;

    &lt;p&gt;How does it induce the vectors? Take a large corpus of contiguous sentences (e.g. books). For each consecutive sentence tuple &lt;code&gt;(x, y, z)&lt;/code&gt;, use an RNN with GRUs to encode the midde sentence, &lt;code&gt;y&lt;/code&gt;. Now use an RNN (with GRU) to decode this encoding and try to predict &lt;code&gt;x&lt;/code&gt;, the sentence before. Do the same thing with another decoder for predicting &lt;code&gt;z&lt;/code&gt;, the sentence after. These two decoders have separate weight matrices but share the weight matrix for the vocabulary.&lt;/p&gt;

    &lt;p&gt;The objective function is the sum of the log-probabilities of the previous and next sentences conditioned on the encoder representation:&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\displaystyle \sum_{t} \log (P(w_{i-1}^t | w_{i-1}^{\le t-1}, h_i)) + \sum_{t} \log (P(w_{i+1}^t | w_{i+1}^{\le t-1}, h_i))&lt;/script&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;Another nice takeaway from this paper was that they were able to expand the vocabulary for the language model from 20k words to 930k words using pre-trained word2vec representations. This was done by first developing the RNN as above and then finding a linear mapping matrix &lt;code&gt;W&lt;/code&gt; where &lt;code&gt;v&#39; ~ Wv&lt;/code&gt; for &lt;code&gt;v&lt;/code&gt; in the word2vec vocabulary and &lt;code&gt;v&#39;&lt;/code&gt; in the RNN vocabulary. Now any word represented in word2vec can also be mapped to the RNN.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script&gt;
  var scripts = document.getElementsByTagName(&quot;script&quot;);
  for (var i = 0; i &lt; scripts.length; i++) {
    var script = scripts[i];
    if (script.type.match(/^math\/tex/)) {
      var text = script.text === &quot;&quot; ? script.innerHTML : script.text;
      var options = script.type.match(/mode\s*=\s*display/) ? {displayMode: true} : {};
      script.insertAdjacentHTML(&quot;beforebegin&quot;,
                                katex.renderToString(text, options));
    }
  }
  document.body.className += &quot; math_finished&quot;;
&lt;/script&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Chandar et al, University of Montreal &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Soyer et al, National Institute of Informatics, Tokyo, Japan &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Kiros et al, University of Toronto &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Google, Efficient Estimation of Word Representations in Vector Space &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 28 Oct 2015 00:00:00 -0400</pubDate>
        <link>http://cinjon.com/papers-leveraging-monolingual</link>
        <guid isPermaLink="true">http://cinjon.com/papers-leveraging-monolingual</guid>
        
        <category>language-models,papers</category>
        
        
      </item>
    
      <item>
        <title>Three Machine Translation Papers</title>
        <description>&lt;p&gt;I’ve been thinking recently about a novel way to do Machine Translation. Here are a few of the papers I’ve explored to see if it’s been done before.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1406.1078v3.pdf&quot;&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation&lt;/a&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;This paper develops a complete RNN encoder - decoder for translating from English to French. It uses this system to rescore hypotheses produced by a phrase-based system.&lt;/p&gt;

    &lt;p&gt;While the author has gone on to develop more advanced systems (see below), there are two major takeaways from this paper. The first is how well the RNN encoder-decoder generates a continuous-space representation of phrases. The 1000-dimensional encodings it produces capture both semantic and syntactic structures helpful in translating between languages.&lt;/p&gt;

    &lt;p&gt;The second major takeaway is the introduction of the Gated Recurrent Unit (GRU). This is similar to the Long Short Term Memory (LSTM) unit but much simpler and seemingly comparable&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. It has a reset gate and an update gate. After non-linearities are applied, the hidden state is a linear combination of the those two gates and the input.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;This famous paper references the above work by Cho as well as the work below by Bahdanau. I might be mistaken, but think that this is the first [successful] result for MT using a deep RNN encoder-decoder. They obtained a really good BLEU score (34.8) using an ensemble of five deep models utilizing 384M parameters and an 8,000 dimensional state.&lt;/p&gt;

    &lt;p&gt;The main takeaway that I had from this paper was the idea of reversing the source string. They found that this was extremely valuable. It makes intuitive sense because if the source string is &lt;code&gt;The dog is here&lt;/code&gt; and the target string is &lt;code&gt;Le chien est ici&lt;/code&gt;, then it’s highly likely that we would want to associate &lt;code&gt;The&lt;/code&gt; with &lt;code&gt;Le&lt;/code&gt;. By encoding that &lt;code&gt;The&lt;/code&gt; last, it has the highest contribution to the encoding. The decoder is then more likely to associate the two correctly.&lt;/p&gt;

    &lt;p&gt;Their hypothesis for this phenomenon is on page 4:&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;… Normally, when we concatenate a source sentence with a target sentence, each word in the source sentence is far from its corresponding word in the target sentence. As a result, the problem has a large “minimal time lag”&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. By reversing the words in the source sentence, the average distance between corresponding words in the source and target language is unchanged. However, the first few words in the source language are now very close to the first few words in the target language, so the problem’s minimal time lag is greatly reduced. Thus, backpropagation has an easier time “establishing communication” between the source sentence and the target sentence…&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;A couple of other notes were that deep LSTM models worked much better than shallow ones. This isn’t surprising and theirs used four layers. The training time, however, was surprising. They had to parallelize the algorithm over an 8-GPU architecture, with the four LSTM layers each having their own GPU. The remaining four were used to run softmax calculations. This resulted in a speed of 6,300 words per second for a training time of ten days.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1409.0473&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;This paper was birthed from the work above by Cho (and he’s a co-author on this one). They conjectured that a bottleneck in the system is that the encoding is a fixed length vector. The input, a sentence, is of variable length. So is the output (another sentence). To get from one to the other, most systems encode the variable-length sequence of words into a fixed length vector and then decode them into the variable-length output. They hypothesized that this caused earlier systems to underperform on longer sentences&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

    &lt;p&gt;Their solution is an &lt;em&gt;attention&lt;/em&gt; model. This has tremendous success jointly learning to translate and align the words correctly.&lt;/p&gt;

    &lt;p&gt;It works by having a bi-directional RNN for the encoder. The forward direction computes a sentence vector as usual (using GRUs). There is another backwards direction that does the same but in reverse. However, there isn’t just one of each - there is a pair for every word in the sentence. For an &lt;code&gt;N&lt;/code&gt;-length sentence, we’ll have &lt;code&gt;N&lt;/code&gt; pairs of these vectors. The &lt;code&gt;jth&lt;/code&gt; forward direction vector is the result of the running the forward RNN up to and including the &lt;code&gt;jth&lt;/code&gt; word. Similarly, the corresponding &lt;code&gt;jth&lt;/code&gt; backward direction vector is the result of running the backward RNN up to and including the &lt;code&gt;jth&lt;/code&gt; word. Each of these pairs are concatenated together and called an Annotation. Think of them as being a summary of the preceding and following words, focused on the nearby words.&lt;/p&gt;

    &lt;p&gt;Now the decoder takes these Annotations as input. It then needs to decide which Annotation to &lt;em&gt;pay attention to&lt;/em&gt;. It does this by computing a probability for each Annotation where the input is influenced by an alignment model. That model scores how well the inputs around each position match with the output at the specified position. It’s a feed-forward neural network that is jointly trained with the whole system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Cho et al, Universite de Montreal &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;A later &lt;a href=&quot;http://arxiv.org/abs/1412.3555&quot;&gt;paper&lt;/a&gt; by Chung et al suggests this. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Sutskever et al, Google &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;S. Hochreiter and J. Schmidhuber: &lt;a href=&quot;http://papers.nips.cc/paper/1215-lstm-can-solve-hard-long-time-lag-problems.pdf&quot;&gt;LSTM can solve hard long time lag problems&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Bahdanau et al, Jacobs University Bremen &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;Cho et al, &lt;a href=&quot;http://arxiv.org/abs/1409.1259&quot;&gt;On the Properties of Neural Machine Translation: Encoder-Decoder Approaches&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 27 Oct 2015 00:00:00 -0400</pubDate>
        <link>http://cinjon.com/three-machine-translation-papers</link>
        <guid isPermaLink="true">http://cinjon.com/three-machine-translation-papers</guid>
        
        <category>machine-translation,papers</category>
        
        
      </item>
    
      <item>
        <title>The Rules of the Game</title>
        <description>&lt;p&gt;Here’s an idea.&lt;/p&gt;

&lt;p&gt;Let’s say you have lots of games of chess written in traditional notation. Now simulate those games and pipe the pixels into an [unsupervised] AI. The goal is not to learn how to beat the best players but just to learn how to play the game well enough to explain it to another.&lt;/p&gt;

&lt;p&gt;In a way, this is like a child watching people play and figuring out all the rules. This is harder for a number of reasons. Two of which are that you have just the board and no language communication. And that’s assuming that we’ve previously taught the AI what are squares, pieces, and the notion of a 1-1 game.&lt;/p&gt;

&lt;p&gt;How would you do this? Has it been done? There is a famous &lt;a href=&quot;https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf&quot;&gt;paper&lt;/a&gt; by the team at DeepMind exploring how to train computers to beat Atari games. For each game, the AI was given video pixel input, the controls, and the rewards / losses (no different than the input and output humans get). With no human-assisted feature selection at all, the system did astonighsly well and surpassed a human expert for three out of the seven games played.&lt;/p&gt;

&lt;p&gt;What I’m prorposing is different. With the Atari games, the machine didn’t have to explain to someone else what it was doing. It just needed to get a better score. Another way to put it is that it couldn’t now go and teach another how to play any of these games. How could it? It doesn’t even have the means to communicate except through play.&lt;/p&gt;

&lt;p&gt;Returning to chess, what is a way to test if someone understands the rules? If it was a kid, we’d ask her to explain how pieces move, how the game is set up, and what ends the game. Once we’d established that, we’d play a game or two to see if she understood everything in combination.&lt;/p&gt;

&lt;p&gt;Is it sufficient to just play the game? No. That’s no different than writing a textbook by combining words in a correct grammatical structure. It’s very hard to train machines to do that correctly but we’re getting very good today at teaching computers to pull this off without them being able to teach the rules of grammar&lt;sup id=&quot;fnref:n&quot;&gt;&lt;a href=&quot;#fn:n&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;There are tests that seem good but break down. One is to leave out some possibilities for pieces and see if the computer picks it up. Say it never sees a rook moving from its original space to the opposite end of its base. How would it know that it could do that? It would be strange to think that it couldn’t if it understood that rooks move in a horizontal fashion and can go up to any number of spaces. And yet it would be strange to think it &lt;em&gt;could&lt;/em&gt; do that given that it has no probabilistic basis for that move. It would be similar to saying that the rook can jump over another one. It happens often enough with another piece (the knight) that that might be in the space of possibilities.&lt;/p&gt;

&lt;p&gt;This might be too unnatural and hard. We as humans don’t even do this - we use language to help guide how we learn games. DeepMind gave the equivalent of the piece movements to the AI (the controls). We could do that. We could also describe the space of moves - horizontal / vertical / diagonal / L-shaped.&lt;/p&gt;

&lt;p&gt;I’d be much more interested in imbuing a system with some faculties for asking questions. Then, when it found out a rook could move five spaces horizontally, it could ask us, “can this piece move six spaces as well?”&lt;/p&gt;

&lt;p&gt;That would be interesting.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:n&quot;&gt;
      &lt;p&gt;This is a non-trivial requirement on the machine. K-12 schools in America don’t even require that students know grammar well enough to teach it to others. They just need to pass tests that determine they grasp the faculties. These tests are often times multiple choice but also written. Training a computer to perform well on that is much easier than what I’m suggesting. &lt;a href=&quot;#fnref:n&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 25 Oct 2015 00:00:00 -0400</pubDate>
        <link>http://cinjon.com/the-rules-of-the-game</link>
        <guid isPermaLink="true">http://cinjon.com/the-rules-of-the-game</guid>
        
        <category>ai,rules,grammer</category>
        
        
      </item>
    
  </channel>
</rss>

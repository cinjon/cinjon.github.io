<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cinjon</title>
    <description>Cinjon&#39;s on the internet
</description>
    <link>http://cinjon.com/</link>
    <atom:link href="http://cinjon.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 27 Oct 2015 13:49:13 -0400</pubDate>
    <lastBuildDate>Tue, 27 Oct 2015 13:49:13 -0400</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>The Rules of the Game</title>
        <description>&lt;p&gt;Here’s an idea.&lt;/p&gt;

&lt;p&gt;Let’s say you have lots of games of chess written in traditional notation. Now simulate those games and pipe the pixels into an [unsupervised] AI. The goal is not to learn how to beat the best players but just to learn how to play the game well enough to explain it to another.&lt;/p&gt;

&lt;p&gt;In a way, this is like a child watching people play and figuring out all the rules. This is harder for a number of reasons. Two of which are that you have just the board and no language communication. And that’s assuming that we’ve previously taught the AI what are squares, pieces, and the notion of a 1-1 game.&lt;/p&gt;

&lt;p&gt;How would you do this? Has it been done? There is a famous &lt;a href=&quot;https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf&quot;&gt;paper&lt;/a&gt; by the team at DeepMind exploring how to train computers to beat Atari games. For each game, the AI was given video pixel input, the controls, and the rewards / losses (no different than the input and output humans get). With no human-assisted feature selection at all, the system did astonighsly well and surpassed a human expert for three out of the seven games played.&lt;/p&gt;

&lt;p&gt;What I’m prorposing is different. With the Atari games, the machine didn’t have to explain to someone else what it was doing. It just needed to get a better score. Another way to put it is that it couldn’t now go and teach another how to play any of these games. How could it? It doesn’t even have the means to communicate except through play.&lt;/p&gt;

&lt;p&gt;Returning to chess, what is a way to test if someone understands the rules? If it was a kid, we’d ask her to explain how pieces move, how the game is set up, and what ends the game. Once we’d established that, we’d play a game or two to see if she understood everything in combination.&lt;/p&gt;

&lt;p&gt;Is it sufficient to just play the game? No. That’s no different than writing a textbook by combining words in a correct grammatical structure. It’s very hard to train machines to do that correctly but we’re getting very good today at teaching computers to pull this off without them being able to teach the rules of grammar&lt;sup id=&quot;fnref:n&quot;&gt;&lt;a href=&quot;#fn:n&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;There are tests that seem good but break down. One is to leave out some possibilities for pieces and see if the computer picks it up. Say it never sees a rook moving from its original space to the opposite end of its base. How would it know that it could do that? It would be strange to think that it couldn’t if it understood that rooks move in a horizontal fashion and can go up to any number of spaces. And yet it would be strange to think it &lt;em&gt;could&lt;/em&gt; do that given that it has no probabilistic basis for that move. It would be similar to saying that the rook can jump over another one. It happens often enough with another piece (the knight) that that might be in the space of possibilities.&lt;/p&gt;

&lt;p&gt;This might be too unnatural and hard. We as humans don’t even do this - we use language to help guide how we learn games. DeepMind gave the equivalent of the piece movements to the AI (the controls). We could do that. We could also describe the space of moves - horizontal / vertical / diagonal / L-shaped.&lt;/p&gt;

&lt;p&gt;I’d be much more interested in imbuing a system with some faculties for asking questions. Then, when it found out a rook could move five spaces horizontally, it could ask us, “can this piece move six spaces as well?”&lt;/p&gt;

&lt;p&gt;That would be interesting.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:n&quot;&gt;
      &lt;p&gt;This is a non-trivial requirement on the machine. K-12 schools in America don’t even require that students know grammar well enough to teach it to others. They just need to pass tests that determine they grasp the faculties. These tests are often times multiple choice but also written. Training a computer to perform well on that is much easier than what I’m suggesting. &lt;a href=&quot;#fnref:n&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 27 Oct 2015 00:00:00 -0400</pubDate>
        <link>http://cinjon.com/the-rules-of-the-game</link>
        <guid isPermaLink="true">http://cinjon.com/the-rules-of-the-game</guid>
        
        <category>ai,rules,grammer</category>
        
        
      </item>
    
      <item>
        <title>Machine Translation Papers Part 1</title>
        <description>&lt;p&gt;I’ve been thinking about a novel idea for Machine Translation. Here are a few of the papers I’ve explored to see if it’s been done before.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1406.1078v3.pdf&quot;&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation&lt;/a&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;This paper develops a complete RNN encoder - decoder for translating from English to French. It uses this system to rescore hypothesis produced by a phrase-based system.&lt;/p&gt;

    &lt;p&gt;While the author has gone on to develop more advanced systems (see below), there are two major takeaways from this paper. The first is how well the RNN encoder-decoder generates a continuous-space representation of phrases. The 1000-dimensional encodings it produces capture both semantic and syntactic structures helpful in translating between languages.&lt;/p&gt;

    &lt;p&gt;The second major takeaway is the introduction of the Gated Recurrent Unit (GRU). This is similar to the Long Short Term Memory (LSTM) unit but much simpler and seemingly comparable &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. It has a reset gate and an update gate. After non-linearities are applied, the hidden state is a linear combination of the those and the input.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;This famous paper references the above work by Cho as well as the below work by Bahdanau. I might be mistaken, but think that this is the first large [successful] result for MT using a deep RNN encoder-decoder. The main result is that they obtained a really good BLEU score (34.8) using an ensemble of five deep models utilizing 384M parameters and an 8,000 dimensional state.&lt;/p&gt;

    &lt;p&gt;The main takeaway that I had from this paper was the idea of reversing the source string. They found that this was extremely valuable. It makes intuitive sense because now if the source string is &lt;code&gt;a,b,c,d&lt;/code&gt; and the target string is &lt;code&gt;t,u,v,w&lt;/code&gt;, then it’s highly likely that we would want to associate &lt;code&gt;a&lt;/code&gt; with &lt;code&gt;t&lt;/code&gt;. By encoding that &lt;code&gt;a&lt;/code&gt; last, it has the highest contribution to the encoding. When the decoder starts reading, it is most likely to associate the two correctly.&lt;/p&gt;

    &lt;p&gt;Their hypothesis for this phenomenon is on page 4:&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;… Normally, when we concatenate a source sentence with a target sentence, each word in the source sentence is far from its corresponding word in the target sentence. As a result, the problem has a large “minimal time lag”&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. By reversing the words in the source sentence, the average distance between corresponding words in the source and target language is unchanged. However, the first few words in the source language are now very close to the first few words in the target language, so the problem’s minimal time lag is greatly reduced. Thus, backpropagation has an easier time “establishing communication” between the source sentence and the target sentence…&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;A couple of other notes here were that they found that deep LSTM models worked much better than shallow ones. This isn’t too surprising and they reported results using four layers. The training time was surprising though. They had to parallelize the algorithm over an 8-GPU architecture with each of the four LSTM layers on a different GPU and then the remaining four used solely for running softmax calculations. This resulted in a speed of 6,300 words per second for a training time of ten days.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1409.0473&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;This paper was birthed from the work above by Cho (and he’s a co-author on this one). They conjectured that a bottleneck in getting the encoder-decoder systems to work better is in the encoding being a fixed length vector. The input, a sentence, is of variable length. So is the output (another sentence). To get from one to the other, most systems encode the variable-length sequence of words into a fixed length vector and then decode them into the variable-length output. They hypothesized that this caused earlier systems to underperform on longer sentences&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

    &lt;p&gt;Their way to get around that is to utilize an &lt;em&gt;attention&lt;/em&gt; model. This has tremendous success jointly learning to translate and align the words correctly. How?&lt;/p&gt;

    &lt;p&gt;It works by having a bi-directional RNN for the encoder. The forward direction computes a sentence vector as usual (using GRUs). There is another backwards direction that does the same but in reverse. However, there isn’t just one of each - there is a pair for every word in the sentence. For an &lt;code&gt;N&lt;/code&gt;-length sentence, we’ll have &lt;code&gt;N&lt;/code&gt; pairs of these vectors. The &lt;code&gt;jth&lt;/code&gt; forward direction vector is the result of the running the forward RNN up to and including the &lt;code&gt;jth&lt;/code&gt; word. Similarly, the corresponding &lt;code&gt;jth&lt;/code&gt; backward direction vector is the result of running the backward RNN up to and including the &lt;code&gt;jth&lt;/code&gt; word. Each of these pairs are concatenated together and called an Annotation. Think of them as being a summary of the preceding and following words, focused on the nearby words.&lt;/p&gt;

    &lt;p&gt;Now the decoder takes these Annotations as input. It then needs to decide which Annotation to &lt;em&gt;pay attention to.&lt;/em&gt; It does this by computing a probability for each Annotation where the input is influenced by an alignment model which scores how well the inputs around each position and the output at the specified position match. This feed-forward neural network is jointly trained with the whole system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Cho et al, Universite de Montreal &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;A later &lt;a href=&quot;http://arxiv.org/abs/1412.3555&quot;&gt;paper&lt;/a&gt; by Chung et al suggests this. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Sutskever et al, Google &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;S. Hochreiter and J. Schmidhuber: &lt;a href=&quot;http://papers.nips.cc/paper/1215-lstm-can-solve-hard-long-time-lag-problems.pdf&quot;&gt;LSTM can solve hard long time lag problems&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Bahdanau et al, Jacobs University Bremen &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;Cho et al, &lt;a href=&quot;http://arxiv.org/abs/1409.1259&quot;&gt;On the Properties of Neural Machine Translation: Encoder-Decoder Approaches&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 27 Oct 2015 00:00:00 -0400</pubDate>
        <link>http://cinjon.com/machine-translation-papers-part-1</link>
        <guid isPermaLink="true">http://cinjon.com/machine-translation-papers-part-1</guid>
        
        <category>machine-translation,papers</category>
        
        
      </item>
    
  </channel>
</rss>
